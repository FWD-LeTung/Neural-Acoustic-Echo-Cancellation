{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ba138",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets[audio]\n",
    "!pip install torchaudio\n",
    "!pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fcfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a535d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Conv1d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88631beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        k_f, k_t = kernel_size\n",
    "        p_f, p_t = padding\n",
    "        self.time_pad = (k_t - 1)\n",
    "        self.freq_pad = p_f\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, stride, padding=(0, 0))\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.time_pad, 0, self.freq_pad, self.freq_pad))\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class GatedTrConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, padding, output_padding=(0,0)):\n",
    "        super().__init__()\n",
    "        k_f, k_t = kernel_size\n",
    "        self.chomp_t = k_t - 1\n",
    "        self.gate_conv = nn.Sequential(nn.Conv2d(in_c, in_c, 1), nn.Tanh())\n",
    "        self.tr_conv = nn.ConvTranspose2d(in_c, out_c, kernel_size, stride, padding, output_padding)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.PReLU()\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x_cat = torch.cat([x, skip], dim=1)\n",
    "        gate = self.gate_conv(x_cat)\n",
    "        x_gated = x_cat * gate\n",
    "        out = self.tr_conv(x_gated)\n",
    "        if self.chomp_t > 0:\n",
    "            out = out[:, :, :, :-self.chomp_t]\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class FT_GRU_Block(nn.Module):\n",
    "    def __init__(self, in_c, hidden_f, hidden_t):\n",
    "        super().__init__()\n",
    "        self.f_gru = nn.GRU(in_c, hidden_f, batch_first=True, bidirectional=True)\n",
    "        self.f_linear = nn.Linear(hidden_f * 2, in_c)\n",
    "        self.t_gru = nn.GRU(in_c, hidden_t, batch_first=True, bidirectional=False)\n",
    "        self.t_linear = nn.Linear(hidden_t, in_c)\n",
    "        self.bn = nn.BatchNorm2d(in_c)\n",
    "        self.act = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, Freq, T = x.shape\n",
    "        x_f = x.permute(0, 3, 2, 1).reshape(-1, Freq, C)\n",
    "        x_f_out, _ = self.f_gru(x_f)\n",
    "        x_f_out = self.f_linear(x_f_out)\n",
    "        x_f_res = x_f_out.reshape(B, T, Freq, C).permute(0, 3, 2, 1)\n",
    "        x = x + x_f_res\n",
    "        x_t = x.permute(0, 2, 3, 1).reshape(-1, T, C)\n",
    "        x_t_out, _ = self.t_gru(x_t)\n",
    "        x_t_out = self.t_linear(x_t_out)\n",
    "        x_t_res = x_t_out.reshape(B, Freq, T, C).permute(0, 3, 1, 2)\n",
    "        x = x + x_t_res\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADModule(nn.Module):\n",
    "    def __init__(self, in_channels=32, freq_bins=8): # Bottleneck Freq = 8 for 257 input\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Sequential(nn.Conv2d(in_channels, 16, 1), nn.BatchNorm2d(16), nn.PReLU())\n",
    "        self.f_gru = nn.GRU(16 * freq_bins, 8, batch_first=True, bidirectional=True)\n",
    "        self.conv1d_block = nn.Sequential(nn.Conv1d(16, 16, 1), nn.BatchNorm1d(16), nn.PReLU())\n",
    "        self.conv1d_out = nn.Conv1d(16, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, Freq, T = x.shape\n",
    "        x = self.conv2d(x)\n",
    "        x = x.permute(0, 3, 1, 2).reshape(B, T, -1)\n",
    "        x, _ = self.f_gru(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d_block(x)\n",
    "        vad_logits = self.conv1d_out(x)\n",
    "        return vad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76110dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFilterOp(nn.Module):\n",
    "    def __init__(self, N_f=3, N_t=3, N_l=1):\n",
    "        super().__init__()\n",
    "        self.N_f, self.N_t, self.N_l = N_f, N_t, N_l\n",
    "        self.k_f = 2 * N_f + 1\n",
    "        self.k_t = N_t + N_l + 1\n",
    "        self.num_neighbors = self.k_f * self.k_t\n",
    "\n",
    "    def forward(self, coarse_spec, filters):\n",
    "        B, C, Freq, T = coarse_spec.shape\n",
    "        spec_padded = F.pad(coarse_spec, (self.N_t, self.N_l, self.N_f, self.N_f))\n",
    "        patches = F.unfold(spec_padded, kernel_size=(self.k_f, self.k_t))\n",
    "        patches = patches.view(B, C, self.num_neighbors, Freq, T)\n",
    "        filters = filters.view(B, 2, self.num_neighbors, Freq, T)\n",
    "        out_r = torch.sum(patches[:,0]*filters[:,0] - patches[:,1]*filters[:,1], dim=1)\n",
    "        out_i = torch.sum(patches[:,0]*filters[:,1] + patches[:,1]*filters[:,0], dim=1)\n",
    "        return torch.stack([out_r, out_i], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseStage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_0 = CausalConvBlock(2, 16, (5,1), (1,1), (2,0))\n",
    "        self.enc_1 = CausalConvBlock(16, 16, (1,5), (1,1), (0,0))\n",
    "        self.enc_2 = CausalConvBlock(16, 16, (6,5), (2,1), (2,0))\n",
    "        self.enc_3 = CausalConvBlock(16, 32, (4,3), (2,1), (1,0))\n",
    "        self.enc_4 = CausalConvBlock(32, 32, (6,5), (2,1), (2,0))\n",
    "        self.enc_5 = CausalConvBlock(32, 32, (5,3), (2,1), (2,0))\n",
    "        self.enc_6 = CausalConvBlock(32, 32, (3,5), (2,1), (1,0))\n",
    "        self.enc_7 = CausalConvBlock(32, 32, (3,3), (1,1), (1,0))\n",
    "        self.gru_0 = FT_GRU_Block(32, 32, 64)\n",
    "        self.gru_1 = FT_GRU_Block(32, 32, 32)\n",
    "        self.vad = VADModule(32, freq_bins=8)\n",
    "        self.dec_0 = GatedTrConvBlock(32+32, 32, (3,3), (1,1), (1,0))\n",
    "        self.dec_1 = GatedTrConvBlock(32+32, 32, (3,5), (2,1), (1,0), output_padding=(1,0))\n",
    "        self.dec_2 = GatedTrConvBlock(32+32, 32, (5,3), (2,1), (2,0), output_padding=(1,0))\n",
    "        self.dec_3 = GatedTrConvBlock(32+32, 32, (6,5), (2,1), (2,0))\n",
    "        self.dec_4 = GatedTrConvBlock(32+32, 16, (4,3), (2,1), (1,0), output_padding=(0,0))\n",
    "        self.dec_5 = GatedTrConvBlock(16+16, 16, (6,5), (2,1), (2,0), output_padding=(1,0))\n",
    "        self.dec_6 = GatedTrConvBlock(16+16, 16, (1,5), (1,1), (0,0))\n",
    "        self.dec_7 = GatedTrConvBlock(16+16, 16, (5,1), (1,1), (2,0))\n",
    "        self.mask_conv = nn.Conv2d(16, 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, mic_cpr, ref_cpr, mic_spec_complex):\n",
    "        x = torch.cat([mic_cpr, ref_cpr], dim=1)\n",
    "        e0 = self.enc_0(x); e1 = self.enc_1(e0); e2 = self.enc_2(e1); e3 = self.enc_3(e2)\n",
    "        e4 = self.enc_4(e3); e5 = self.enc_5(e4); e6 = self.enc_6(e5); e7 = self.enc_7(e6)\n",
    "        g0 = self.gru_0(e7); g1 = self.gru_1(g0)\n",
    "        vad_out = self.vad(g1)\n",
    "        d0 = self.dec_0(g1, e7); d1 = self.dec_1(d0, e6); d2 = self.dec_2(d1, e5); d3 = self.dec_3(d2, e4)\n",
    "        d4 = self.dec_4(d3, e3); d5 = self.dec_5(d4, e2); d6 = self.dec_6(d5, e1); d7 = self.dec_7(d6, e0)\n",
    "        mask = self.mask_conv(d7)\n",
    "        pred_r = mic_spec_complex[:,0]*mask[:,0] - mic_spec_complex[:,1]*mask[:,1]\n",
    "        pred_i = mic_spec_complex[:,0]*mask[:,1] + mic_spec_complex[:,1]*mask[:,0]\n",
    "        coarse_out = torch.stack([pred_r, pred_i], dim=1)\n",
    "        return coarse_out, vad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd43126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineStage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_0 = CausalConvBlock(3, 16, (5,1), (1,1), (2,0))\n",
    "        self.enc_1 = CausalConvBlock(16, 16, (1,5), (1,1), (0,0))\n",
    "        self.enc_2 = CausalConvBlock(16, 32, (6,5), (2,1), (2,0))\n",
    "        self.enc_3 = CausalConvBlock(32, 32, (4,3), (2,1), (1,0))\n",
    "        self.enc_4 = CausalConvBlock(32, 64, (6,5), (2,1), (2,0))\n",
    "        self.enc_5 = CausalConvBlock(64, 64, (5,3), (2,1), (2,0))\n",
    "        self.enc_6 = CausalConvBlock(64, 64, (3,5), (2,1), (1,0))\n",
    "        self.enc_7 = CausalConvBlock(64, 64, (3,3), (1,1), (1,0))\n",
    "        self.gru_0 = FT_GRU_Block(64, 64, 128)\n",
    "        self.gru_1 = FT_GRU_Block(64, 64, 64)\n",
    "        self.dec_0 = GatedTrConvBlock(64+64, 64, (3,3), (1,1), (1,0))\n",
    "        self.dec_1 = GatedTrConvBlock(64+64, 64, (3,5), (2,1), (1,0), output_padding=(1,0))\n",
    "        self.dec_2 = GatedTrConvBlock(64+64, 64, (5,3), (2,1), (2,0), output_padding=(1,0))\n",
    "        self.dec_3 = GatedTrConvBlock(64+32, 32, (6,5), (2,1), (2,0))\n",
    "        self.dec_4 = GatedTrConvBlock(32+32, 32, (4,3), (2,1), (1,0), output_padding=(0,0))\n",
    "        self.dec_5 = GatedTrConvBlock(32+32, 16, (6,5), (2,1), (2,0), output_padding=(1,0))\n",
    "        self.dec_6 = GatedTrConvBlock(16+16, 16, (1,5), (1,1), (0,0))\n",
    "        self.dec_7 = GatedTrConvBlock(16+16, 16, (5,1), (1,1), (2,0))\n",
    "        self.df_conv = nn.Conv2d(16, 70, kernel_size=1)\n",
    "        self.df_op = DeepFilterOp(N_f=3, N_t=3, N_l=1)\n",
    "\n",
    "    def forward(self, mic_cpr, ref_cpr, coarse_cpr, coarse_out_complex):\n",
    "        x = torch.cat([mic_cpr, ref_cpr, coarse_cpr], dim=1)\n",
    "        e0 = self.enc_0(x); e1 = self.enc_1(e0); e2 = self.enc_2(e1); e3 = self.enc_3(e2)\n",
    "        e4 = self.enc_4(e3); e5 = self.enc_5(e4); e6 = self.enc_6(e5); e7 = self.enc_7(e6)\n",
    "        g0 = self.gru_0(e7); g1 = self.gru_1(g0)\n",
    "        d0 = self.dec_0(g1, e7); d1 = self.dec_1(d0, e6); d2 = self.dec_2(d1, e5); d3 = self.dec_3(d2, e4)\n",
    "        d4 = self.dec_4(d3, e3); d5 = self.dec_5(d4, e2); d6 = self.dec_6(d5, e1); d7 = self.dec_7(d6, e0)\n",
    "        df_coef = self.df_conv(d7)\n",
    "        fine_out = self.df_op(coarse_out_complex, df_coef)\n",
    "        return fine_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb927f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.coarse_stage = CoarseStage()\n",
    "        self.fine_stage = FineStage()\n",
    "        self.alpha = 0.3\n",
    "\n",
    "    def compress(self, complex_spec):\n",
    "        # input: [B, 2, F, T]\n",
    "        mag = torch.sqrt(complex_spec[:, 0]**2 + complex_spec[:, 1]**2 + 1e-8)\n",
    "        mag_compressed = torch.pow(mag, self.alpha)\n",
    "        return mag_compressed.unsqueeze(1) # [B, 1, F, T]\n",
    "\n",
    "    def forward(self, mic_complex, ref_complex):\n",
    "        mic_cpr = self.compress(mic_complex)\n",
    "        ref_cpr = self.compress(ref_complex)\n",
    "        coarse_out, vad_prob = self.coarse_stage(mic_cpr, ref_cpr, mic_complex)e\n",
    "        coarse_cpr = self.compress(coarse_out)\n",
    "        fine_out = self.fine_stage(mic_cpr, ref_cpr, coarse_cpr, coarse_out)\n",
    "        \n",
    "        return fine_out, vad_prob, coarse_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517073f",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c82774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEC_Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, n_fft=512, win_length=320, hop_length=160, duration=8):\n",
    "        # Note: Paper uses duration 8s\n",
    "        self.dataset = hf_dataset\n",
    "        self.n_fft, self.win_length, self.hop_length = n_fft, win_length, hop_length\n",
    "        self.max_len = int(16000 * duration)\n",
    "        self.window = torch.hann_window(self.win_length)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.dataset)\n",
    "\n",
    "    def process_len(self, wav_array):\n",
    "        wav = torch.from_numpy(wav_array).float()\n",
    "        if wav.ndim == 1: wav = wav.unsqueeze(0)\n",
    "        # Random crop or Pad\n",
    "        if wav.shape[1] > self.max_len:\n",
    "            start = random.randint(0, wav.shape[1] - self.max_len)\n",
    "            return wav[:, start:start + self.max_len], start\n",
    "        return torch.nn.functional.pad(wav, (0, self.max_len - wav.shape[1])), 0\n",
    "\n",
    "    def get_stft(self, wav):\n",
    "        stft_complex = torch.stft(wav, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                                  win_length=self.win_length, window=self.window,\n",
    "                                  center=True, return_complex=True)\n",
    "        stft_real_imag = torch.view_as_real(stft_complex).squeeze(0) # [F, T, 2]\n",
    "        return stft_real_imag.permute(2, 0, 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        mic_wav, start = self.process_len(item['mic']['array'])\n",
    "        ref_wav, _ = self.process_len(item['ref']['array'])\n",
    "        clean_wav, _ = self.process_len(item['clean']['array'])\n",
    "\n",
    "        vad_full = torch.tensor(item['vad_label'], dtype=torch.long)\n",
    "\n",
    "        # Align VAD label with STFT frames\n",
    "        num_frames = self.get_stft(mic_wav).shape[2] # Dimension T is now at index 2\n",
    "        start_frame = start // self.hop_length\n",
    "        vad_label = vad_full[start_frame : start_frame + num_frames]\n",
    "\n",
    "        if vad_label.shape[0] < num_frames:\n",
    "            vad_label = torch.nn.functional.pad(vad_label, (0, num_frames - vad_label.shape[0]))\n",
    "        elif vad_label.shape[0] > num_frames:\n",
    "            vad_label = vad_label[:num_frames]\n",
    "\n",
    "        # Returns: [2, F, T] for audio, [T] for VAD\n",
    "        return self.get_stft(mic_wav), self.get_stft(ref_wav), self.get_stft(clean_wav), vad_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba83fd1",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tspnn_loss(est_fine, est_coarse, target, vad_logits, vad_labels, w=0.3, beta=0.06):\n",
    "    \"\"\"\n",
    "    Combined Loss: Eq (7) and (9)\n",
    "    L = [w * L_coarse + (1-w) * L_fine] + beta * L_vad\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Phase-aware MAE Loss (Eq 6)\n",
    "    def phase_aware_mae(pred, tgt):\n",
    "        # pred, tgt: [B, 2, F, T]\n",
    "        # Magnitude\n",
    "        mag_pred = torch.sqrt(pred[:,0]**2 + pred[:,1]**2 + 1e-8)\n",
    "        mag_tgt = torch.sqrt(tgt[:,0]**2 + tgt[:,1]**2 + 1e-8)\n",
    "        loss_mag = F.l1_loss(mag_pred, mag_tgt)\n",
    "        \n",
    "        # Real & Imag\n",
    "        loss_real = F.l1_loss(pred[:,0], tgt[:,0])\n",
    "        loss_imag = F.l1_loss(pred[:,1], tgt[:,1])\n",
    "        \n",
    "        return loss_mag + loss_real + loss_imag\n",
    "\n",
    "    loss_coarse = phase_aware_mae(est_coarse, target)\n",
    "    loss_fine = phase_aware_mae(est_fine, target)\n",
    "    loss_vad = F.cross_entropy(vad_logits, vad_labels)\n",
    "    \n",
    "    # 3. Final Weighted Sum\n",
    "    loss_main = w * loss_coarse + (1 - w) * loss_fine\n",
    "    total_loss = loss_main + beta * loss_vad\n",
    "    \n",
    "    return total_loss, loss_coarse, loss_fine, loss_vad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfe31a",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_to_mag_db(stft_tensor):\n",
    "    mag = torch.sqrt(stft_tensor[0]**2 + stft_tensor[1]**2 + 1e-9)\n",
    "    return 20 * torch.log10(mag + 1e-9)\n",
    "\n",
    "def visual_check_tspnn(model, device, step, val_dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mic, ref, clean, vad_lab = val_dataset[0]\n",
    "        # Add batch dim: [1, 2, F, T]\n",
    "        mic = mic.unsqueeze(0).to(device)\n",
    "        ref = ref.unsqueeze(0).to(device)\n",
    "        clean = clean.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        fine_out, vad_out, coarse_out = model(mic, ref)\n",
    "        \n",
    "        img_mic = stft_to_mag_db(mic[0])\n",
    "        img_coarse = stft_to_mag_db(coarse_out[0])\n",
    "        img_fine = stft_to_mag_db(fine_out[0])\n",
    "        img_clean = stft_to_mag_db(clean)\n",
    "        \n",
    "        # Plot\n",
    "        imgs = [img_mic, img_coarse, img_fine, img_clean]\n",
    "        titles = [\"Microphone (Input)\", \"Coarse Output (P-AEC)\", \"Fine Output (R-AEC)\", \"Clean (Target)\"]\n",
    "        \n",
    "        fig, axs = plt.subplots(4, 1, figsize=(10, 14), facecolor='white')\n",
    "        for i, (img, title) in enumerate(zip(imgs, titles)):\n",
    "            axs[i].imshow(img.cpu().numpy(), origin='lower', aspect='auto', cmap='magma')\n",
    "            axs[i].set_title(title)\n",
    "            axs[i].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"Inference_Comparison\": wandb.Image(fig)}, step=step)\n",
    "        plt.close(fig)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416949ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    CONFIG = {\n",
    "        \"repo_id\": \"PandaLT/microsoft-AEC-vad-dataset\",\n",
    "        \"n_fft\": 512, \n",
    "        \"win_length\": 320, \n",
    "        \"hop_length\": 160,\n",
    "        \"batch_size\": 16, \n",
    "        \"lr\": 1e-3,       \n",
    "        \"epochs\": 10,\n",
    "        \"w_loss\": 0.3,    \n",
    "        \"beta_loss\": 0.06,\n",
    "        \"val_interval\": 100,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    output_dir = \"/content/drive/MyDrive/AEC/TSPNN_Checkpoints\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    set_seed(CONFIG['seed'])\n",
    "\n",
    "    wandb.init(project=\"TSPNN_AEC_Implementation\", config=CONFIG)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Loading Dataset from HuggingFace...\")\n",
    "    full_dataset = load_dataset(CONFIG['repo_id'], split='train')\n",
    "    \n",
    "    # Simple split (Take 200 for val, rest for train)\n",
    "    val_data = full_dataset.select(range(200))\n",
    "    train_data = full_dataset.select(range(200, len(full_dataset)))\n",
    "    \n",
    "    params = {\n",
    "        \"n_fft\": CONFIG[\"n_fft\"], \n",
    "        \"win_length\": CONFIG[\"win_length\"], \n",
    "        \"hop_length\": CONFIG[\"hop_length\"],\n",
    "        \"duration\": 8 # TSPNN paper uses 8s\n",
    "    }\n",
    "    \n",
    "    train_ds = AEC_Dataset(train_data, **params)\n",
    "    val_ds = AEC_Dataset(val_data, **params)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "    print(\"Initializing TSPNN Model...\")\n",
    "    model = TSPNN().to(device)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    global_step = 0\n",
    "    print(\"Starting Training...\")\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (mic, ref, clean, vad_labels) in enumerate(train_loader):\n",
    "            # Inputs: [B, 2, F, T]\n",
    "            mic, ref, clean = mic.to(device), ref.to(device), clean.to(device)\n",
    "            vad_labels = vad_labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            fine_out, vad_logits, coarse_out = model(mic, ref)\n",
    "            \n",
    "            # [cite_start]Loss [cite: 450]\n",
    "            loss, l_coarse, l_fine, l_vad = tspnn_loss(\n",
    "                fine_out, coarse_out, clean, vad_logits, vad_labels,\n",
    "                w=CONFIG['w_loss'], beta=CONFIG['beta_loss']\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping is often helpful for GRUs\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if global_step % 10 == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": loss.item(),\n",
    "                    \"loss_coarse\": l_coarse.item(),\n",
    "                    \"loss_fine\": l_fine.item(),\n",
    "                    \"loss_vad\": l_vad.item(),\n",
    "                    \"epoch\": epoch\n",
    "                }, step=global_step)\n",
    "                print(f\"Step {global_step} | Loss: {loss.item():.4f} | Fine: {l_fine.item():.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            if global_step % CONFIG['val_interval'] == 0:\n",
    "                model.eval()\n",
    "                val_loss_accum = 0\n",
    "                with torch.no_grad():\n",
    "                    for v_mic, v_ref, v_clean, v_vad in val_loader:\n",
    "                        v_mic, v_ref, v_clean = v_mic.to(device), v_ref.to(device), v_clean.to(device)\n",
    "                        v_vad = v_vad.to(device)\n",
    "                        \n",
    "                        v_fine, v_logits, v_coarse = model(v_mic, v_ref)\n",
    "                        v_loss, _, _, _ = tspnn_loss(v_fine, v_coarse, v_clean, v_logits, v_vad)\n",
    "                        val_loss_accum += v_loss.item()\n",
    "                \n",
    "                avg_val_loss = val_loss_accum / len(val_loader)\n",
    "                wandb.log({\"val_loss\": avg_val_loss}, step=global_step)\n",
    "                print(f\"--- Validation Loss: {avg_val_loss:.4f} ---\")\n",
    "                \n",
    "                # Visual Check\n",
    "                visual_check_tspnn(model, device, global_step, val_ds)\n",
    "                \n",
    "                # Save checkpoint\n",
    "                torch.save(model.state_dict(), f\"{output_dir}/tspnn_step_{global_step}.pth\")\n",
    "                model.train()\n",
    "        \n",
    "        # End of Epoch\n",
    "        scheduler.step(epoch_loss / len(train_loader))\n",
    "        print(f\">>> End of Epoch {epoch+1}\")\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
