{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets[audio]\n",
    "!pip install torchaudio\n",
    "!pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f57f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.Conv1d, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f82a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualMHSA(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        attn_mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n",
    "        x_out, _ = self.mha(x, x, x, attn_mask=attn_mask)\n",
    "        return x_out\n",
    "\n",
    "class ConformerConvModule(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pointwise_conv1 = nn.Conv1d(d_model, d_model * 2, kernel_size=1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=(kernel_size-1)//2, groups=d_model)\n",
    "        self.batch_norm = nn.GroupNorm(num_groups=1, num_channels=d_model)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(self.pointwise_conv2(self.activation(self.batch_norm(self.depthwise_conv(self.glu(self.pointwise_conv1(x)))))))\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_model*expansion_factor)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_model*expansion_factor, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.layer2(self.dropout1(self.activation(self.layer1(x)))))\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn1 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.conv_module = ConformerConvModule(d_model, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.self_attn = CasualMHSA(d_model, n_head, dropout=dropout)\n",
    "        self.ffn2 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model); self.norm4 = nn.LayerNorm(d_model)\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = x + 0.5 * self.ffn1(self.norm1(x))\n",
    "        x = x + self.conv_module(self.norm2(x))\n",
    "        x = x + self.self_attn(self.norm3(x))\n",
    "        x = x + 0.5 * self.ffn2(self.norm4(x))\n",
    "        return self.final_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEC_V2(nn.Module):\n",
    "    def __init__(self, d_model=128, n_fft=512, n_head=8, num_layers=4, kernel_size=15):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.n_freq = n_fft // 2 + 1\n",
    "        input_dim = self.n_freq * 4\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            ConformerBlock(d_model, n_head, kernel_size=kernel_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Nhánh 1: AEC Mask (Đầu ra chính)\n",
    "        self.mask_proj = nn.Linear(d_model, self.n_freq * 2)\n",
    "        \n",
    "        # Nhánh 2: VAD Classifier (Đầu ra phụ trợ - Giai đoạn 2)\n",
    "        # Class 0: Silence/Echo, Class 1: Nearend Speech\n",
    "        self.vad_proj = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, mic_stft, ref_stft):\n",
    "        B, F, T, C = mic_stft.shape\n",
    "        mic_flat = mic_stft.permute(0, 2, 1, 3).reshape(B, T, F * 2)\n",
    "        ref_flat = ref_stft.permute(0, 2, 1, 3).reshape(B, T, F * 2)\n",
    "        \n",
    "        x = torch.cat([mic_flat, ref_flat], dim=2)\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Shared Backbone (Conformer Layers)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Branch 1: AEC Masking\n",
    "        mask = self.mask_proj(x)\n",
    "        mask = mask.view(B, T, F, 2).permute(0, 2, 1, 3)\n",
    "        mic_real, mic_imag = mic_stft[..., 0], mic_stft[..., 1]\n",
    "        mask_real, mask_imag = mask[..., 0], mask[..., 1]\n",
    "        est_real = mic_real * mask_real - mic_imag * mask_imag\n",
    "        est_imag = mic_real * mask_imag + mic_imag * mask_real\n",
    "        est_stft = torch.stack([est_real, est_imag], dim=-1)\n",
    "        \n",
    "        # Branch 2: VAD Logits (Trả về B, T, 2)\n",
    "        vad_logits = self.vad_proj(x)\n",
    "        \n",
    "        return est_stft, vad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df27ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_STFTDataset_V2(Dataset):\n",
    "    def __init__(self, hf_dataset, n_fft=512, win_length=320, hop_length=160, duration=10):\n",
    "        self.dataset = hf_dataset\n",
    "        self.n_fft, self.win_length, self.hop_length = n_fft, win_length, hop_length\n",
    "        self.max_len = int(16000 * duration)\n",
    "        self.window = torch.hann_window(self.win_length)\n",
    "\n",
    "    def __len__(self): return len(self.dataset)\n",
    "\n",
    "    def process_len(self, wav_array):\n",
    "        wav = torch.from_numpy(wav_array).float()\n",
    "        if wav.ndim == 1: wav = wav.unsqueeze(0)\n",
    "        if wav.shape[1] > self.max_len:\n",
    "            start = random.randint(0, wav.shape[1] - self.max_len)\n",
    "            return wav[:, start:start + self.max_len], start\n",
    "        return torch.nn.functional.pad(wav, (0, self.max_len - wav.shape[1])), 0\n",
    "\n",
    "    def get_stft(self, wav):\n",
    "        # Sửa lỗi return_complex=False bằng cách dùng view_as_real\n",
    "        stft_complex = torch.stft(wav, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                                  win_length=self.win_length, window=self.window,\n",
    "                                  center=True, return_complex=True)\n",
    "        return torch.view_as_real(stft_complex).squeeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Xử lý audio và lấy vị trí start để cắt VAD label tương ứng\n",
    "        mic_wav, start = self.process_len(item['mic']['array'])\n",
    "        ref_wav, _ = self.process_len(item['ref']['array'])\n",
    "        clean_wav, _ = self.process_len(item['clean']['array'])\n",
    "        \n",
    "        # Lấy nhãn VAD từ dataset (dạng List)\n",
    "        vad_full = torch.tensor(item['vad_label'], dtype=torch.long)\n",
    "        \n",
    "        # Cắt nhãn VAD khớp với đoạn audio (10ms mỗi nhãn)\n",
    "        num_frames = self.get_stft(mic_wav).shape[1] # Số khung T\n",
    "        start_frame = start // self.hop_length\n",
    "        vad_label = vad_full[start_frame : start_frame + num_frames]\n",
    "        \n",
    "        # Padding nhãn VAD nếu thiếu do làm tròn\n",
    "        if vad_label.shape[0] < num_frames:\n",
    "            vad_label = torch.nn.functional.pad(vad_label, (0, num_frames - vad_label.shape[0]))\n",
    "        elif vad_label.shape[0] > num_frames:\n",
    "            vad_label = vad_label[:num_frames]\n",
    "\n",
    "        return self.get_stft(mic_wav), self.get_stft(ref_wav), self.get_stft(clean_wav), vad_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65675bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_task_loss(est, target, vad_logits, vad_labels, beta=0.1):\n",
    "    \"\"\"\n",
    "    L = L_AEC + beta * L_VAD\n",
    "    L_AEC: MAE Magnitude + MAE Complex (từ bài báo NWPU/ByteAudio)\n",
    "    L_VAD: CrossEntropy (từ bài báo NWPU) [cite: 178]\n",
    "    \"\"\"\n",
    "    # 1. AEC Loss\n",
    "    mae = nn.L1Loss()\n",
    "    mag_est = torch.sqrt(est[..., 0]**2 + est[..., 1]**2 + 1e-9)\n",
    "    mag_target = torch.sqrt(target[..., 0]**2 + target[..., 1]**2 + 1e-9)\n",
    "    loss_aec = mae(mag_est, mag_target) + mae(est, target)\n",
    "    \n",
    "    # 2. VAD Loss (CrossEntropy trên trục T)\n",
    "    # vad_logits: (B, T, 2) -> (B, 2, T) cho CrossEntropy\n",
    "    # vad_labels: (B, T)\n",
    "    loss_vad = nn.CrossEntropyLoss()(vad_logits.transpose(1, 2), vad_labels)\n",
    "    \n",
    "    total_loss = loss_aec + beta * loss_vad\n",
    "    return total_loss, loss_aec, loss_vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_to_mag_db(stft_tensor):\n",
    "    mag = torch.sqrt(stft_tensor[..., 0]**2 + stft_tensor[..., 1]**2 + 1e-9)\n",
    "    return 20 * torch.log10(mag + 1e-9)\n",
    "\n",
    "def visual_check_v2(model, device, step, val_dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mic, ref, clean, vad_lab = val_dataset[0]\n",
    "        mic, ref = mic.unsqueeze(0).to(device), ref.unsqueeze(0).to(device)\n",
    "        est_stft, _ = model(mic, ref)\n",
    "        \n",
    "        imgs = [stft_to_mag_db(mic[0]), stft_to_mag_db(ref[0]), stft_to_mag_db(clean), stft_to_mag_db(est_stft[0])]\n",
    "        titles = [\"Microphone\", \"Reference\", \"Clean (Target)\", f\"Estimate (Step {step})\"]\n",
    "        fig, axs = plt.subplots(4, 1, figsize=(10, 12), facecolor='black')\n",
    "        for i, img in enumerate(imgs):\n",
    "            axs[i].imshow(img.cpu().numpy(), origin='lower', aspect='auto', cmap='magma')\n",
    "            axs[i].set_title(titles[i], color='white')\n",
    "            axs[i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"Inference_Progress\": wandb.Image(fig)}, step=step)\n",
    "        plt.close(fig)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    CONFIG = {\n",
    "        \"repo_id\": \"PandaLT/microsoft-AEC-dataset\", # Dataset mới có vad_label\n",
    "        \"n_fft\": 512, \"win_length\": 320, \"hop_length\": 160,\n",
    "        \"d_model\": 128, \"batch_size\": 32, \"lr\": 2e-4, \"epochs\": 10,\n",
    "        \"warmup_pct\": 0.05, \"beta\": 0.1, # Trọng số VAD loss \n",
    "        \"val_interval\": 100, \"seed\": 42\n",
    "    }\n",
    "\n",
    "    set_seed(CONFIG['seed'])\n",
    "    wandb.init(project=\"AEC_Stage2_MultiTask_VAD\", config=CONFIG)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Loading Multi-task Dataset...\")\n",
    "    full_dataset = load_dataset(CONFIG['repo_id'], split='train')\n",
    "    val_data = full_dataset.select(range(500))\n",
    "    train_data = full_dataset.select(range(500, len(full_dataset)))\n",
    "\n",
    "    params = {k: CONFIG[k] for k in [\"n_fft\", \"win_length\", \"hop_length\"]}\n",
    "    val_ds = HF_STFTDataset_V2(val_data, **params)\n",
    "    train_loader = DataLoader(HF_STFTDataset_V2(train_data, **params), batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    model = AEC_V2(d_model=CONFIG['d_model'], n_fft=CONFIG['n_fft']).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['lr'], total_steps=total_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        print(f\"\\n>>> Epoch {epoch+1}\")\n",
    "        for mic, ref, clean, vad_label in train_loader:\n",
    "            mic, ref, clean, vad_label = mic.to(device), ref.to(device), clean.to(device), vad_label.to(device)\n",
    "\n",
    "            est_stft, vad_logits = model(mic, ref)\n",
    "            total_loss, loss_aec, loss_vad = multi_task_loss(est_stft, clean, vad_logits, vad_label, beta=CONFIG['beta'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0:\n",
    "                wandb.log({\n",
    "                    \"total_loss\": total_loss.item(),\n",
    "                    \"aec_loss\": loss_aec.item(),\n",
    "                    \"vad_loss\": loss_vad.item(),\n",
    "                    \"lr\": scheduler.get_last_lr()[0]\n",
    "                }, step=global_step)\n",
    "\n",
    "            if global_step % CONFIG['val_interval'] == 0:\n",
    "                visual_check_v2(model, device, global_step, val_ds)\n",
    "                torch.save(model.state_dict(), f\"aec_v2_step_{global_step}.pth\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
