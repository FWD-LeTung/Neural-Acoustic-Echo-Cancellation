{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab78bac",
   "metadata": {},
   "source": [
    "Docstring for source.TSNN_coarse_fine_Stage.ipynb\n",
    "\n",
    "Gated TrConv2D\n",
    "Casual Conv2D\n",
    "FT-GRU\n",
    "\n",
    "Corse Stage\n",
    "Fine Stage\n",
    "VAD\n",
    "Deep Filter\n",
    "\n",
    "TDC, Magnitude Compress\n",
    "===> TSPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a9a87",
   "metadata": {},
   "source": [
    "### Fundamental Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "faba68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv2D + BN + PReLU với Causal Padding trên trục thời gian.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        # kernel_size: (Freq, Time)\n",
    "        k_f, k_t = kernel_size\n",
    "        s_f, s_t = stride\n",
    "        p_f, p_t = padding # p_t ở đây là padding thông thường, ta sẽ xử lý causal riêng\n",
    "\n",
    "        # Causal padding: Chỉ pad bên trái trục Time\n",
    "        self.time_pad = (k_t - 1) \n",
    "        self.freq_pad = p_f # Pad đều trục Freq\n",
    "\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, stride, padding=(0, 0))\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]\n",
    "        # Pad: (Left, Right, Top, Bottom) -> (Time_Left, Time_Right, Freq_Top, Freq_Bottom)\n",
    "        x = F.pad(x, (self.time_pad, 0, self.freq_pad, self.freq_pad))\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class GatedTrConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Hình 1(B): Gated Transpose Convolution.\n",
    "    Fix: Thêm logic 'chomp' để cắt bỏ phần thừa trục thời gian do Transpose Conv sinh ra.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, padding, output_padding=(0,0)):\n",
    "        super().__init__()\n",
    "        k_f, k_t = kernel_size\n",
    "        self.chomp_t = k_t - 1 # Lượng thừa ra do Transpose Conv\n",
    "        \n",
    "        self.gate_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, in_c, kernel_size=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.tr_conv = nn.ConvTranspose2d(\n",
    "            in_c, out_c, kernel_size, stride, padding, output_padding\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.PReLU()\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x_cat = torch.cat([x, skip], dim=1) \n",
    "        \n",
    "        # 2. Gate mechanism\n",
    "        gate = self.gate_conv(x_cat)\n",
    "        x_gated = x_cat * gate\n",
    "        \n",
    "        # 3. Transpose Conv\n",
    "        out = self.tr_conv(x_gated)\n",
    "        if self.chomp_t > 0:\n",
    "            out = out[:, :, :, :-self.chomp_t]\n",
    "            \n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "    \n",
    "class FT_GRU_Block(nn.Module):\n",
    "    def __init__(self, in_c, hidden_f, hidden_t):\n",
    "        super().__init__()\n",
    "        self.f_gru = nn.GRU(in_c, hidden_f, batch_first=True, bidirectional=True)\n",
    "        self.f_linear = nn.Linear(hidden_f * 2, in_c)\n",
    "        self.t_gru = nn.GRU(in_c, hidden_t, batch_first=True, bidirectional=False)\n",
    "        self.t_linear = nn.Linear(hidden_t, in_c)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(in_c)\n",
    "        self.act = nn.PReLU()\n",
    "    def forward(self,x):\n",
    "        B, C, F, T = x.shape\n",
    "        x_f = x.permute(0, 3, 2, 1).reshape(-1, F, C)\n",
    "        x_f_out, _ = self.f_gru(x_f) # [B*T, F, hidden_f*2]\n",
    "        x_f_out = self.f_linear(x_f_out) # [B*T, F, C]\n",
    "        x_f_res = x_f_out.reshape(B, T, F, C).permute(0, 3, 2, 1)\n",
    "        x = x + x_f_res\n",
    "\n",
    "        x_t = x.permute(0, 2, 3, 1).reshape(-1, T, C)\n",
    "        x_t_out, _ = self.t_gru(x_t) # [B*F, T, hidden_t]\n",
    "        x_t_out = self.t_linear(x_t_out)\n",
    "        x_t_res = x_t_out.reshape(B, F, T, C).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = x + x_t_res\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cba7d7",
   "metadata": {},
   "source": [
    "### VAD block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADModule(nn.Module):\n",
    "    def __init__(self, in_channels=32, freq_bins=5): \n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.f_gru = nn.GRU(16 * freq_bins, 8, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.conv1d_block = nn.Sequential(\n",
    "            nn.Conv1d(16, 16, kernel_size=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv1d_out = nn.Conv1d(16, 2, kernel_size=1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, Freq, T = x.shape  \n",
    "        x = self.conv2d(x) \n",
    "        x = x.permute(0, 3, 1, 2).reshape(B, T, -1)\n",
    "        x, _ = self.f_gru(x) \n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d_block(x)\n",
    "        vad_logits = self.conv1d_out(x)\n",
    "        return vad_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc67424",
   "metadata": {},
   "source": [
    "### Deep Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eced4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFilterOp(nn.Module):\n",
    "    def __init__(self, N_f=3, N_t=3, N_l=1):\n",
    "        super().__init__()\n",
    "        self.N_f = N_f\n",
    "        self.N_t = N_t\n",
    "        self.N_l = N_l\n",
    "        self.k_f = 2 * N_f + 1\n",
    "        self.k_t = N_t + N_l + 1\n",
    "        self.num_neighbors = self.k_f * self.k_t\n",
    "\n",
    "    def forward(self, coarse_spec, filters):\n",
    "        # coarse_spec: [B, 2, Freq, T]\n",
    "        B, C, Freq, T = coarse_spec.shape \n",
    "        # Pad Freq: N_f trên, N_f dưới\n",
    "        spec_padded = F.pad(coarse_spec, (self.N_t, self.N_l, self.N_f, self.N_f))\n",
    "        # Unfold\n",
    "        patches = F.unfold(spec_padded, kernel_size=(self.k_f, self.k_t))\n",
    "        # Reshape patches sử dụng biến Freq\n",
    "        patches = patches.view(B, C, self.num_neighbors, Freq, T) # <--- DÙNG Freq\n",
    "        \n",
    "        filters = filters.view(B, 2, self.num_neighbors, Freq, T) # <--- DÙNG Freq\n",
    "        filter_r = filters[:, 0]\n",
    "        filter_i = filters[:, 1]\n",
    "        \n",
    "        spec_r = patches[:, 0]\n",
    "        spec_i = patches[:, 1]\n",
    "        \n",
    "        out_r = torch.sum(spec_r * filter_r - spec_i * filter_i, dim=1)\n",
    "        out_i = torch.sum(spec_r * filter_i + spec_i * filter_r, dim=1)\n",
    "        \n",
    "        out = torch.stack([out_r, out_i], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd84d2",
   "metadata": {},
   "source": [
    "### Coarse and Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e37280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseStage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder \n",
    "        self.enc_0 = CausalConvBlock(2, 16, (5,1), (1,1), (2,0))\n",
    "        self.enc_1 = CausalConvBlock(16, 16, (1,5), (1,1), (0,0))\n",
    "        self.enc_2 = CausalConvBlock(16, 16, (6,5), (2,1), (2,0))\n",
    "        self.enc_3 = CausalConvBlock(16, 32, (4,3), (2,1), (1,0)) # Out: 32\n",
    "        self.enc_4 = CausalConvBlock(32, 32, (6,5), (2,1), (2,0))\n",
    "        self.enc_5 = CausalConvBlock(32, 32, (5,3), (2,1), (2,0))\n",
    "        self.enc_6 = CausalConvBlock(32, 32, (3,5), (2,1), (1,0))\n",
    "        self.enc_7 = CausalConvBlock(32, 32, (3,3), (1,1), (1,0))\n",
    "\n",
    "        self.gru_0 = FT_GRU_Block(32, 32, 64)\n",
    "        self.gru_1 = FT_GRU_Block(32, 32, 32)\n",
    "        self.vad = VADModule(32, freq_bins=8) \n",
    "        \n",
    "        # Decoder \n",
    "        # dec_0 in: gru(32) + enc_7(32) = 64\n",
    "        self.dec_0 = GatedTrConvBlock(32+32, 32, (3,3), (1,1), (1,0)) \n",
    "        \n",
    "        # dec_1 in: dec_0(32) + enc_6(32) = 64\n",
    "        self.dec_1 = GatedTrConvBlock(32+32, 32, (3,5), (2,1), (1,0), output_padding=(1,0))\n",
    "        \n",
    "        # dec_2 in: dec_1(32) + enc_5(32) = 64\n",
    "        self.dec_2 = GatedTrConvBlock(32+32, 32, (5,3), (2,1), (2,0), output_padding=(1,0))\n",
    "        \n",
    "        # dec_3 in: dec_2(32) + enc_4(32) = 64\n",
    "        self.dec_3 = GatedTrConvBlock(32+32, 32, (6,5), (2,1), (2,0))\n",
    "        \n",
    "        # dec_4 in: dec_3(32) + enc_3(32) = 64. (FIXED: Old was 32+16)\n",
    "        self.dec_4 = GatedTrConvBlock(32+32, 16, (4,3), (2,1), (1,0), output_padding=(0,0))\n",
    "        \n",
    "        # dec_5 in: dec_4(16) + enc_2(16) = 32\n",
    "        self.dec_5 = GatedTrConvBlock(16+16, 16, (6,5), (2,1), (2,0), output_padding=(1,0))\n",
    "        \n",
    "        # dec_6 in: dec_5(16) + enc_1(16) = 32\n",
    "        self.dec_6 = GatedTrConvBlock(16+16, 16, (1,5), (1,1), (0,0))\n",
    "        \n",
    "        # dec_7 in: dec_6(16) + enc_0(16) = 32. (FIXED: Old was 16+2)\n",
    "        self.dec_7 = GatedTrConvBlock(16+16, 16, (5,1), (1,1), (2,0)) \n",
    "        \n",
    "        self.mask_conv = nn.Conv2d(16, 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, mic_cpr, ref_cpr, mic_spec_complex):\n",
    "        x = torch.cat([mic_cpr, ref_cpr], dim=1)\n",
    "        \n",
    "        e0 = self.enc_0(x)\n",
    "        e1 = self.enc_1(e0)\n",
    "        e2 = self.enc_2(e1)\n",
    "        e3 = self.enc_3(e2)\n",
    "        e4 = self.enc_4(e3)\n",
    "        e5 = self.enc_5(e4)\n",
    "        e6 = self.enc_6(e5)\n",
    "        e7 = self.enc_7(e6) \n",
    "        \n",
    "        g0 = self.gru_0(e7)\n",
    "        g1 = self.gru_1(g0)\n",
    "        \n",
    "        vad_out = self.vad(g1)\n",
    "        \n",
    "        d0 = self.dec_0(g1, e7)\n",
    "        d1 = self.dec_1(d0, e6)\n",
    "        d2 = self.dec_2(d1, e5)\n",
    "        d3 = self.dec_3(d2, e4)\n",
    "        d4 = self.dec_4(d3, e3)\n",
    "        d5 = self.dec_5(d4, e2)\n",
    "        d6 = self.dec_6(d5, e1)\n",
    "        d7 = self.dec_7(d6, e0)\n",
    "        \n",
    "        mask = self.mask_conv(d7)\n",
    "        pred_r = mic_spec_complex[:,0]*mask[:,0] - mic_spec_complex[:,1]*mask[:,1]\n",
    "        pred_i = mic_spec_complex[:,0]*mask[:,1] + mic_spec_complex[:,1]*mask[:,0]\n",
    "        coarse_out = torch.stack([pred_r, pred_i], dim=1)\n",
    "        return coarse_out, vad_out\n",
    "\n",
    "class FineStage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder \n",
    "        self.enc_0 = CausalConvBlock(3, 16, (5,1), (1,1), (2,0))\n",
    "        self.enc_1 = CausalConvBlock(16, 16, (1,5), (1,1), (0,0))\n",
    "        self.enc_2 = CausalConvBlock(16, 32, (6,5), (2,1), (2,0))\n",
    "        self.enc_3 = CausalConvBlock(32, 32, (4,3), (2,1), (1,0))\n",
    "        self.enc_4 = CausalConvBlock(32, 64, (6,5), (2,1), (2,0)) # Out: 64\n",
    "        self.enc_5 = CausalConvBlock(64, 64, (5,3), (2,1), (2,0))\n",
    "        self.enc_6 = CausalConvBlock(64, 64, (3,5), (2,1), (1,0))\n",
    "        self.enc_7 = CausalConvBlock(64, 64, (3,3), (1,1), (1,0))\n",
    "\n",
    "        # GRU \n",
    "        self.gru_0 = FT_GRU_Block(64, 64, 128)\n",
    "        self.gru_1 = FT_GRU_Block(64, 64, 64)\n",
    "        \n",
    "        # Decoder \n",
    "        # dec_0 in: gru(64) + enc_7(64) = 128\n",
    "        self.dec_0 = GatedTrConvBlock(64+64, 64, (3,3), (1,1), (1,0))\n",
    "        \n",
    "        # dec_1 in: dec_0(64) + enc_6(64) = 128\n",
    "        self.dec_1 = GatedTrConvBlock(64+64, 64, (3,5), (2,1), (1,0), output_padding=(1,0))\n",
    "        \n",
    "        # dec_2 in: dec_1(64) + enc_5(64) = 128\n",
    "        self.dec_2 = GatedTrConvBlock(64+64, 64, (5,3), (2,1), (2,0), output_padding=(1,0))\n",
    "        \n",
    "        # dec_3 in: dec_2(64) + enc_4(64) = 128. (FIXED: Old was 64+32)\n",
    "        self.dec_3 = GatedTrConvBlock(64+64, 32, (6,5), (2,1), (2,0))\n",
    "        \n",
    "        # dec_4 in: dec_3(32) + enc_3(32) = 64\n",
    "        self.dec_4 = GatedTrConvBlock(32+32, 32, (4,3), (2,1), (1,0), output_padding=(0,0))\n",
    "        \n",
    "        # dec_5 in: dec_4(32) + enc_2(32) = 64\n",
    "        self.dec_5 = GatedTrConvBlock(32+32, 16, (6,5), (2,1), (2,0), output_padding=(1,0))\n",
    "        \n",
    "        # dec_6 in: dec_5(16) + enc_1(16) = 32\n",
    "        self.dec_6 = GatedTrConvBlock(16+16, 16, (1,5), (1,1), (0,0))\n",
    "        \n",
    "        # dec_7 in: dec_6(16) + enc_0(16) = 32\n",
    "        self.dec_7 = GatedTrConvBlock(16+16, 16, (5,1), (1,1), (2,0))\n",
    "        \n",
    "        self.df_conv = nn.Conv2d(16, 70, kernel_size=1)\n",
    "        self.df_op = DeepFilterOp(N_f=3, N_t=3, N_l=1)\n",
    "\n",
    "    def forward(self, mic_cpr, ref_cpr, coarse_out_cpr, coarse_out_complex):\n",
    "        x = torch.cat([mic_cpr, ref_cpr, coarse_out_cpr], dim=1)\n",
    "        e0 = self.enc_0(x)\n",
    "        e1 = self.enc_1(e0)\n",
    "        e2 = self.enc_2(e1)\n",
    "        e3 = self.enc_3(e2)\n",
    "        e4 = self.enc_4(e3)\n",
    "        e5 = self.enc_5(e4)\n",
    "        e6 = self.enc_6(e5)\n",
    "        e7 = self.enc_7(e6)\n",
    "        \n",
    "        g0 = self.gru_0(e7)\n",
    "        g1 = self.gru_1(g0)\n",
    "        \n",
    "        d0 = self.dec_0(g1, e7)\n",
    "        d1 = self.dec_1(d0, e6)\n",
    "        d2 = self.dec_2(d1, e5)\n",
    "        d3 = self.dec_3(d2, e4)\n",
    "        d4 = self.dec_4(d3, e3)\n",
    "        d5 = self.dec_5(d4, e2)\n",
    "        d6 = self.dec_6(d5, e1)\n",
    "        d7 = self.dec_7(d6, e0)\n",
    "        \n",
    "        df_coef = self.df_conv(d7)\n",
    "        fine_out = self.df_op(coarse_out_complex, df_coef)\n",
    "        return fine_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ece9c",
   "metadata": {},
   "source": [
    "### Wrapper Model (TSPNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f0c1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.coarse_stage = CoarseStage()\n",
    "        self.fine_stage = FineStage()\n",
    "        self.alpha = 0.3 # Compression factor [cite: 100]\n",
    "\n",
    "    def compress(self, complex_spec):\n",
    "        # complex_spec: [B, 2, F, T]\n",
    "        mag = torch.sqrt(complex_spec[:, 0]**2 + complex_spec[:, 1]**2 + 1e-8)\n",
    "        mag_compressed = torch.pow(mag, self.alpha)\n",
    "        return mag_compressed.unsqueeze(1) # [B, 1, F, T]\n",
    "\n",
    "    def forward(self, mic_complex, ref_complex):\n",
    "        \"\"\"\n",
    "        Input: Spectrograms [B, 2, F, T] (Real/Imag)\n",
    "        \"\"\"\n",
    "        # 1. Prepare Compressed Inputs\n",
    "        mic_cpr = self.compress(mic_complex)\n",
    "        ref_cpr = self.compress(ref_complex)\n",
    "        \n",
    "        # 2. Coarse Stage\n",
    "        coarse_out, vad_prob = self.coarse_stage(mic_cpr, ref_cpr, mic_complex)\n",
    "        \n",
    "        # 3. Prepare Input for Fine Stage\n",
    "        coarse_cpr = self.compress(coarse_out)\n",
    "        \n",
    "        # 4. Fine Stage\n",
    "        fine_out = self.fine_stage(mic_cpr, ref_cpr, coarse_cpr, coarse_out)\n",
    "        \n",
    "        return fine_out, vad_prob, coarse_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8cb6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TSPNN...\n",
      "Input shape: torch.Size([2, 2, 257, 100])\n",
      "Coarse out: torch.Size([2, 2, 257, 100])\n",
      "VAD out: torch.Size([2, 2, 100])\n",
      "Fine out: torch.Size([2, 2, 257, 100])\n",
      "Total Parameters: 1411936\n"
     ]
    }
   ],
   "source": [
    "B, n_freq, T = 2, 257, 100\n",
    "mic = torch.randn(B, 2, n_freq, T) # Real, Imag\n",
    "ref = torch.randn(B, 2, n_freq, T)\n",
    "    \n",
    "model = TSPNN()\n",
    "print(\"Initializing TSPNN...\")\n",
    "    \n",
    "    # Forward pass\n",
    "fine, vad, coarse = model(mic, ref)\n",
    "    \n",
    "print(f\"Input shape: {mic.shape}\")\n",
    "print(f\"Coarse out: {coarse.shape}\")\n",
    "print(f\"VAD out: {vad.shape}\")\n",
    "print(f\"Fine out: {fine.shape}\")\n",
    "    \n",
    "    # Check parameters count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43856698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.Conv1d, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_STFTDataset_V2(Dataset):\n",
    "    def __init__(self, hf_dataset, n_fft=512, win_length=320, hop_length=160, duration=10):\n",
    "        self.dataset = hf_dataset\n",
    "        self.n_fft, self.win_length, self.hop_length = n_fft, win_length, hop_length\n",
    "        self.max_len = int(16000 * duration)\n",
    "        self.window = torch.hann_window(self.win_length)\n",
    "\n",
    "    def __len__(self): return len(self.dataset)\n",
    "\n",
    "    def process_len(self, wav_array):\n",
    "        wav = torch.from_numpy(wav_array).float()\n",
    "        if wav.ndim == 1: wav = wav.unsqueeze(0)\n",
    "        if wav.shape[1] > self.max_len:\n",
    "            start = random.randint(0, wav.shape[1] - self.max_len)\n",
    "            return wav[:, start:start + self.max_len], start\n",
    "        return torch.nn.functional.pad(wav, (0, self.max_len - wav.shape[1])), 0\n",
    "\n",
    "    def get_stft(self, wav):\n",
    "        # Sửa lỗi return_complex=False bằng cách dùng view_as_real\n",
    "        stft_complex = torch.stft(wav, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                                  win_length=self.win_length, window=self.window,\n",
    "                                  center=True, return_complex=True)\n",
    "        return torch.view_as_real(stft_complex).squeeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Xử lý audio và lấy vị trí start để cắt VAD label tương ứng\n",
    "        mic_wav, start = self.process_len(item['mic']['array'])\n",
    "        ref_wav, _ = self.process_len(item['ref']['array'])\n",
    "        clean_wav, _ = self.process_len(item['clean']['array'])\n",
    "        \n",
    "        # Lấy nhãn VAD từ dataset (dạng List)\n",
    "        vad_full = torch.tensor(item['vad_label'], dtype=torch.long)\n",
    "        \n",
    "        # Cắt nhãn VAD khớp với đoạn audio (10ms mỗi nhãn)\n",
    "        num_frames = self.get_stft(mic_wav).shape[1] # Số khung T\n",
    "        start_frame = start // self.hop_length\n",
    "        vad_label = vad_full[start_frame : start_frame + num_frames]\n",
    "        \n",
    "        # Padding nhãn VAD nếu thiếu do làm tròn\n",
    "        if vad_label.shape[0] < num_frames:\n",
    "            vad_label = torch.nn.functional.pad(vad_label, (0, num_frames - vad_label.shape[0]))\n",
    "        elif vad_label.shape[0] > num_frames:\n",
    "            vad_label = vad_label[:num_frames]\n",
    "\n",
    "        return self.get_stft(mic_wav), self.get_stft(ref_wav), self.get_stft(clean_wav), vad_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04eff2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neural-AEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
