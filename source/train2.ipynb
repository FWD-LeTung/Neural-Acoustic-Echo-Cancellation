{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d55ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d63e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"74431323c68300cd7507575e9532ee9077cd6a0a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1327cf",
   "metadata": {},
   "source": [
    "### Conformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbbb55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualMHSA(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        attn_mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n",
    "        x_out, _ = self.mha(x, x, x, attn_mask=attn_mask)\n",
    "        return x_out\n",
    "\n",
    "class ConformerConvModule(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pointwise_conv1 = nn.Conv1d(d_model, d_model * 2, kernel_size=1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=(kernel_size-1)//2, groups=d_model)\n",
    "        self.batch_norm = nn.GroupNorm(num_groups=1, num_channels=d_model)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(self.pointwise_conv2(self.activation(self.batch_norm(self.depthwise_conv(self.glu(self.pointwise_conv1(x)))))))\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_model*expansion_factor)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_model*expansion_factor, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.layer2(self.dropout1(self.activation(self.layer1(x)))))\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn1 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.conv_module = ConformerConvModule(d_model, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.self_attn = CasualMHSA(d_model, n_head, dropout=dropout)\n",
    "        self.ffn2 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model); self.norm4 = nn.LayerNorm(d_model)\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = x + 0.5 * self.ffn1(self.norm1(x))\n",
    "        x = x + self.conv_module(self.norm2(x))\n",
    "        x = x + self.self_attn(self.norm3(x))\n",
    "        x = x + 0.5 * self.ffn2(self.norm4(x))\n",
    "        return self.final_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b49db",
   "metadata": {},
   "source": [
    "### Neural AEC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb1a73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEC(nn.Module):\n",
    "    def __init__(self,d_model=128, n_fft=512, n_head=8, num_layers=4, kernel_size=15 ):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.n_freq = n_fft //2 + 1\n",
    "        input_dim = self.n_freq*4\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            ConformerBlock(d_model,n_head, kernel_size=kernel_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.mask_proj = nn.Linear(d_model, self.n_freq*2)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, mic_stft, ref_stft):\n",
    "        B, F, T, C = mic_stft.shape\n",
    "        mic_flat = mic_stft.permute(0, 2, 1, 3).reshape(B, T, F*2)\n",
    "        ref_flat = ref_stft.permute(0, 2, 1, 3).reshape(B, T, F*2)\n",
    "\n",
    "        x = torch.cat([mic_flat, ref_flat], dim=2)\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        for layer in self.layer:\n",
    "            x = layer(x)\n",
    "        \n",
    "        mask = self.mask_proj(x)\n",
    "        mask = mask.view(B,T,F,2).permute(0,2,1,3)\n",
    "        mic_real = mic_stft[..., 0]\n",
    "        mic_imag = mic_stft[..., 1]\n",
    "        mask_real = mask[..., 0]\n",
    "        mask_imag = mask[..., 1]\n",
    "\n",
    "        est_real = mic_real*mask_real - mic_imag*mask_imag\n",
    "        est_imag = mic_real*mask_imag + mic_imag*mask_real\n",
    "        est_stft = torch.stack([est_real, est_imag], dim=-1)\n",
    "\n",
    "        return est_stft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa22f7",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e32d7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTDataset(Dataset):\n",
    "    def __init__(self, mic, ref, clean, split=\"train\", n_fft=512, hop_length=256, duration=10):\n",
    "        mics = sorted(glob.glob(os.path.join(mic, \"*.wav\")))\n",
    "        refs = sorted(glob.glob(os.path.join(ref, \"*.wav\")))\n",
    "        cleans = sorted(glob.glob(os.path.join(clean, \"*.wav\")))\n",
    "        \n",
    "        if split == \"val\":\n",
    "            self.mic_files = mics[:500]\n",
    "            self.ref_files = refs[:500]\n",
    "            self.clean_files = cleans[:500]\n",
    "        else:\n",
    "            self.mic_files = mics[500:]\n",
    "            self.ref_files = refs[500:]\n",
    "            self.clean_files = cleans[500:]\n",
    "        \n",
    "        self.n_fft, self.hop_length = n_fft, hop_length\n",
    "        self.max_len = int(16000 * duration)\n",
    "        print(f\"{split.upper()} Dataset: {len(self.mic_files)} samples.\")\n",
    "\n",
    "    def __len__(self): return len(self.mic_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mic, _ = torchaudio.load(self.mic_files[idx])\n",
    "        ref, _ = torchaudio.load(self.ref_files[idx])\n",
    "        clean, _ = torchaudio.load(self.clean_files[idx])\n",
    "\n",
    "        # Padding/Clipping logic\n",
    "        for audio in [mic, ref, clean]:\n",
    "            if audio.shape[1] > self.max_len: pass\n",
    "\n",
    "            \n",
    "        window = torch.hann_window(self.n_fft)\n",
    "        def get_stft(x): return torch.stft(x, self.n_fft, self.hop_length, window=window, return_complex=False).squeeze(0)\n",
    "        \n",
    "        return get_stft(mic), get_stft(ref), get_stft(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283249de",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe7c2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_to_mag_db(stft_tensor):\n",
    "    mag = torch.sqrt(stft_tensor[..., 0]**2 + stft_tensor[..., 1]**2 + 1e-9)\n",
    "    return 20 * torch.log10(mag + 1e-9)\n",
    "\n",
    "def visual_check(model, device, epoch, config):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Load fixed samples\n",
    "        def load_fixed(p):\n",
    "            wav, _ = torchaudio.load(p)\n",
    "            return torch.stft(wav, config['n_fft'], config['hop_length'], \n",
    "                              window=torch.hann_window(config['n_fft']), return_complex=False).to(device)\n",
    "        \n",
    "        mic = load_fixed(config['fixed_mic_path'])\n",
    "        ref = load_fixed(config['fixed_ref_path'])\n",
    "        clean = load_fixed(config['fixed_clean_path'])\n",
    "        \n",
    "        est = model(mic, ref)\n",
    "        \n",
    "        imgs = [stft_to_mag_db(mic[0]), stft_to_mag_db(ref[0]), \n",
    "                stft_to_mag_db(clean[0]), stft_to_mag_db(est[0])]\n",
    "        titles = [\"Microphone\", \"Reference\", \"Clean (Target)\", f\"Estimate (Epoch {epoch})\"]\n",
    "        \n",
    "        fig, axs = plt.subplots(4, 1, figsize=(10, 12), facecolor='black')\n",
    "        for i, img in enumerate(imgs):\n",
    "            axs[i].imshow(img.cpu().numpy(), origin='lower', aspect='auto', cmap='magma')\n",
    "            axs[i].set_title(titles[i], color='white', fontsize=10)\n",
    "            axs[i].axis('off') # Xóa trục x và y\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"Spectrogram_Visual\": wandb.Image(fig)}, step=epoch)\n",
    "        plt.close(fig)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bb85a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9786b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    CONFIG = {\n",
    "        \"n_fft\": 400, \n",
    "        \"hop_length\": 200, \n",
    "        \"d_model\": 128, \n",
    "        \"num_layers\": 4,\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 1e-5, \n",
    "        \"epochs\": 1, \n",
    "        \"log_step\": 10, \n",
    "        \"test_step\": 125,\n",
    "        \"data_path\": {\n",
    "            \"mic\": \"D:/AEC-Challenge/datasets/synthetic/nearend_mic_signal\",\n",
    "            \"ref\": \"D:/AEC-Challenge/datasets/synthetic/farend_speech\",\n",
    "            \"clean\": \"D:/AEC-Challenge/datasets/synthetic/nearend_speech\"\n",
    "        },\n",
    "        \"fixed_mic_path\": \"source/nearend_mic_fileid_1609.wav\",\n",
    "        \"fixed_ref_path\": \"source/farend_speech_fileid_1609.wav\",\n",
    "        \"fixed_clean_path\": \"source/nearend_speech_fileid_1609.wav\"\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"AEC_STFT_Conformer\", config=CONFIG)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load Train & Val\n",
    "    train_loader = DataLoader(STFTDataset(**CONFIG['data_path'], split=\"train\"), batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(STFTDataset(**CONFIG['data_path'], split=\"val\"), batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = AEC(d_model=CONFIG['d_model'], num_layers=CONFIG['num_layers']).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    loss_fn = nn.MSELoss() \n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        for i, (mic, ref, clean) in enumerate(train_loader):\n",
    "            mic, ref, clean = mic.to(device), ref.to(device), clean.to(device)\n",
    "            est = model(mic, ref)\n",
    "            \n",
    "            # Hybrid Loss\n",
    "            loss_complex = loss_fn(est, clean)\n",
    "            loss_mag = loss_fn(torch.abs(torch.view_as_complex(est)), torch.abs(torch.view_as_complex(clean)))\n",
    "            loss = loss_complex + loss_mag\n",
    "            \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            train_loss_epoch += loss.item()\n",
    "            if i % CONFIG['log_step'] == 0: wandb.log({\"train_loss\": loss.item()})\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for mic, ref, clean in val_loader:\n",
    "                mic, ref, clean = mic.to(device), ref.to(device), clean.to(device)\n",
    "                val_loss += loss_fn(model(mic, ref), clean).item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"val_loss\": avg_val_loss, \"epoch\": epoch + 1})\n",
    "        print(f\"Epoch {epoch+1} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        if (epoch + 1) % CONFIG['test_step'] == 0:\n",
    "            visual_check(model, device, epoch + 1, CONFIG)\n",
    "            torch.save(model.state_dict(), f\"aec_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ed012d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-lion-4</strong> at: <a href='https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer/runs/jk2n7q43' target=\"_blank\">https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer/runs/jk2n7q43</a><br> View project at: <a href='https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer' target=\"_blank\">https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251218_180458-jk2n7q43\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Neural-AEC\\source\\wandb\\run-20251218_181218-6pshteh5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer/runs/6pshteh5' target=\"_blank\">lively-armadillo-5</a></strong> to <a href='https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer' target=\"_blank\">https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer/runs/6pshteh5' target=\"_blank\">https://wandb.ai/lethanhtung9319-hanoi-university-of-science-and-technology/AEC_STFT_Conformer/runs/6pshteh5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: 9500 samples.\n",
      "VAL Dataset: 500 samples.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.9.1+cu130) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m model.train()\n\u001b[32m     35\u001b[39m train_loss_epoch = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mSTFTDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     mic, _ = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmic_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     ref, _ = torchaudio.load(\u001b[38;5;28mself\u001b[39m.ref_files[idx])\n\u001b[32m     25\u001b[39m     clean, _ = torchaudio.load(\u001b[38;5;28mself\u001b[39m.clean_files[idx])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchaudio\\__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchaudio\\_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Import torchcodec here to provide clear error if not available\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\__init__.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Note: usort wants to put Frame and FrameBatch after decoders and samplers,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# but that results in circular import.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, encoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\decoders\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decoder_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_cuda_backend  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\_core\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AudioStreamMetadata,\n\u001b[32m     10\u001b[39m     ContainerMetadata,\n\u001b[32m     11\u001b[39m     get_container_metadata,\n\u001b[32m     12\u001b[39m     get_container_metadata_from_header,\n\u001b[32m     13\u001b[39m     VideoStreamMetadata,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     _add_video_stream,\n\u001b[32m     17\u001b[39m     _get_backend_details,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     seek_to_pts,\n\u001b[32m     46\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\_core\\_metadata.py:16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _get_container_json_metadata,\n\u001b[32m     18\u001b[39m     _get_stream_json_metadata,\n\u001b[32m     19\u001b[39m     create_from_file,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     23\u001b[39m SPACES = \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStreamMetadata\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\_core\\ops.py:104\u001b[39m\n\u001b[32m    100\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m os.add_dll_directory(\u001b[38;5;28mstr\u001b[39m(ffmpeg_dir))  \u001b[38;5;66;03m# that's the actual CM\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m expose_ffmpeg_dlls():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     ffmpeg_major_version, core_library_path = \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n\u001b[32m    109\u001b[39m create_from_file = torch._dynamo.disallow_in_graph(\n\u001b[32m    110\u001b[39m     torch.ops.torchcodec_ns.create_from_file.default\n\u001b[32m    111\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\_core\\ops.py:75\u001b[39m, in \u001b[36mload_torchcodec_shared_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     68\u001b[39m         exceptions.append((ffmpeg_major_version, e))\n\u001b[32m     70\u001b[39m traceback = (\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     73\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     76\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m         versions 4, 5, 6, 7, and 8. On Windows, ensure you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve installed\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m         the \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfull-shared\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m version which ships DLLs.\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[33m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is not compatible with\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[33m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[33m         table:\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[33m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[33m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.9.1+cu130) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\Users\\Admin.ADMIN-PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback]."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbeaf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cu130\n",
      "Is GPU available: True\n",
      "CUDA Version built with PyTorch: 13.0\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Is GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version built with PyTorch: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
